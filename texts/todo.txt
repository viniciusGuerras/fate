TODO - 
	- learn SIMD and vectorization
	- try to keep the memory as contiguous as possible for the tensors
TO IMPLEMET:
	partially done - tensor_squeeze — remove dimensions of size 1
	partially done - tensor_unsqueeze / tensor_expand_dims — add dimension
	partially done - tensor_transpose — swap dimensions
	partially done - tensor_permute — reorder axes arbitrarily
	p- tensor_neg — -x
	p- tensor_abs — fabsf(x)
	p- tensor_exp, tensor_log, tensor_sqrt

	- tensor_sin, tensor_cos, tensor_tan
	- tensor_relu, tensor_sigmoid, tensor_tanh
	- tensor_floor, tensor_ceil, tensor_round
	- tensor_square, tensor_pow_scalar
	- tensor_sum — sum over an axis or all axes
	- tensor_mean
	- tensor_prod
	- tensor_max, tensor_min
	- tensor_argmax, tensor_argmin
	- tensor_std, tensor_var
	- tensor_matmul — matrix multiply (2D or broadcasted)
	- tensor_dot — dot product
	- tensor_outer — outer product
	- tensor_mv / tensor_mm — matrix-vector/matrix-matrix
	- tensor_det, tensor_inv (optional, advanced)
	- tensor_zeros(shape, ndim)
	- tensor_ones(shape, ndim)
	- tensor_full(shape, ndim, value)
	- tensor_arange(start, end, step)
	- tensor_linspace(start, end, num)
	- tensor_eye(n) — identity matrix
	- tensor_random_uniform, tensor_random_normal
	- tensor_save(path) / tensor_load(path) — binary I/O
	- tensor_tofile(FILE*), tensor_fromfile(FILE*)
	- tensor_is_contiguous
	- tensor_make_contiguous
	- tensor_broadcast_to
	- tensor_slice, tensor_index, tensor_view
	- double, int32_t, int64_t variants
	- dtype enum in Tensor struct

Type-generic ops (via macros or function pointers)
(Advanced) Autograd System
(Optional) Performance Enhancements


